{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy Transformer Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example extracted from:\n",
    "    - https://explosion.ai/blog/spacy-transformers\n",
    "    - https://github.com/explosion/spacy-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import torch\n",
    "import numpy\n",
    "from numpy.testing import assert_almost_equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_using_gpu = spacy.prefer_gpu()\n",
    "if is_using_gpu:\n",
    "    torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_trf_bertbaseuncased_lg\")\n",
    "doc = nlp(\"Here is some text to encode.\")\n",
    "assert doc.tensor.shape == (7, 768)  # Always has one row per token\n",
    "doc._.trf_word_pieces_  # String values of the wordpieces\n",
    "doc._.trf_word_pieces  # Wordpiece IDs (note: *not* spaCy's hash values!)\n",
    "doc._.trf_alignment  # Alignment between spaCy tokens and wordpieces\n",
    "# The raw transformer output has one row per wordpiece.\n",
    "assert len(doc._.trf_last_hidden_state) == len(doc._.trf_word_pieces)\n",
    "# To avoid losing information, we calculate the doc.tensor attribute such that\n",
    "# the sum-pooled vectors match (apart from numeric error)\n",
    "assert_almost_equal(doc.tensor.sum(axis=0), doc._.trf_last_hidden_state.sum(axis=0), decimal=5)\n",
    "span = doc[2:4]\n",
    "# Access the tensor from Span elements (especially helpful for sentences)\n",
    "assert numpy.array_equal(span.tensor, doc.tensor[2:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7342854\n",
      "0.43365785\n"
     ]
    }
   ],
   "source": [
    "# .vector and .similarity use the transformer outputs\n",
    "apple1 = nlp(\"Apple shares rose on the news.\")\n",
    "apple2 = nlp(\"Apple sold fewer iPhones this quarter.\")\n",
    "apple3 = nlp(\"Apple pie is delicious.\")\n",
    "print(apple1[0].similarity(apple2[0]))  # 0.73428553\n",
    "print(apple1[0].similarity(apple3[0]))  # 0.43365782"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Example Closer to Our HSBC Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4038766\n",
      "0.6132948\n"
     ]
    }
   ],
   "source": [
    "# .vector and .similarity use the transformer outputs\n",
    "hsbc1 = nlp(\"OECD is directing financial flows towards all SDGs.\")\n",
    "hsbc2 = nlp(\"Water in rivers flow from high to low altitute.\")\n",
    "hsbc3 = nlp(\"HSBC is supporting the flow of finance towards a green transition.\")\n",
    "print(hsbc1[0].similarity(hsbc2[0]))\n",
    "print(hsbc1[0].similarity(hsbc3[0])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important Aspect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important features are the raw outputs of the transformer, which can be accessed at doc._.trf_outputs.last_hidden_state . This variable gives you a tensor with one row per wordpiece token. The doc.tensor attribute gives you one row per spaCy token, which is useful if you're working on token-level tasks such as part-of-speech tagging or spelling correction. We've taken care to calculate an alignment between the models' various wordpiece tokenization schemes and spaCy'slilininguisisticicalyly-motivivatedtokenizizatioion, with a weighting scheme that ensures that no information is lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.48589775, -0.4306911 , -0.23172405, ..., -0.80624217,\n",
       "         0.05815674, -0.14307418],\n",
       "       [-0.10500746, -0.6504953 , -0.03901236, ..., -0.22964466,\n",
       "        -0.37916154,  0.7867269 ],\n",
       "       [ 0.07593302, -0.4535252 ,  0.24927792, ...,  0.28133318,\n",
       "        -0.4593692 ,  0.8306134 ],\n",
       "       ...,\n",
       "       [ 0.854046  ,  0.04648603,  0.0145665 , ...,  0.26930487,\n",
       "         0.07758062,  0.57686764],\n",
       "       [ 0.8994547 ,  0.66806537,  0.2916338 , ..., -0.6893833 ,\n",
       "        -0.829394  ,  0.5464675 ],\n",
       "       [ 0.4891531 ,  0.2633897 , -0.57394785, ...,  0.21986733,\n",
       "        -0.3543096 , -0.43486243]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc.tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc._.trf_outputs.last_hidden_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_oecdflows",
   "language": "python",
   "name": "venv_oecdflows"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
